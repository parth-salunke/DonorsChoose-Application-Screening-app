{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\parth\\\\DonorsChoose-Application-Screening-app\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Project_path = \"E:\\parth\\DonorsChoose-Application-Screening-app\"\n",
    "os.chdir(Project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\parth\\\\DonorsChoose-Application-Screening-app'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'config/config.yaml'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"config/config.yaml\"\"\"\n",
    "\n",
    "# artifacts_root: artifacts\n",
    "\n",
    "# data_ingestion:\n",
    "#   root_dir: artifacts/data_ingestion\n",
    "#   source_URL: https://drive.google.com/file/d/1-4mgFvbEEvCuIar7eas29F5k8hfbRFn3/view?usp=sharing\n",
    "#   local_data_file: artifacts/data_ingestion/dataset.zip\n",
    "#   unzip_dir: artifacts/data_ingestion\n",
    "\n",
    "# data_cleaning:\n",
    "#   root_dir: artifacts/data_cleaning\n",
    "#   local_data_trainfile: artifacts/data_ingestion/dataset/train1000.csv\n",
    "#   local_data_resourcefile: artifacts/data_ingestion/dataset/resource1000.csv\n",
    "#   local_data_stopwordsfile: artifacts/data_ingestion/stopwords-en.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"src/entity/config_entity -  added entity for data clean \"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataCleanConfig:\n",
    "    root_dir: Path\n",
    "    local_data_trainfile: Path\n",
    "    local_data_resourcefile: Path\n",
    "    local_data_stopwordsfile: Path\n",
    "    save_clean_datafile: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"data config manager it will create config object for data clean \"\"\"\n",
    "\n",
    "from donorschoose.constants import *\n",
    "from donorschoose.utils.common import read_yaml, create_directories \n",
    "# from donorschoose.entity.config_entity import DataCleanConfig\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        # self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    \n",
    "    def get_data_clean_config(self) -> DataCleanConfig:\n",
    "        config = self.config.data_cleaning\n",
    "        print(config)\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_clean_config = DataCleanConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            local_data_trainfile=config.local_data_trainfile,\n",
    "            local_data_resourcefile=config.local_data_resourcefile,\n",
    "            local_data_stopwordsfile =config.local_data_stopwordsfile,\n",
    "            save_clean_datafile = config.save_clean_datafile\n",
    "        )\n",
    "        return data_clean_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "\n",
    "# csv_path = Path(Project_path + r\"\\artifacts\\data_ingestion\\dataset\\train1000.csv\")\n",
    "# df = read_csv(csv_path)\n",
    "# print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from donorschoose import logger\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from donorschoose.entity.config_entity import DataCleanConfig\n",
    "# from donorschoose.config.configuration import ConfigurationManager\n",
    "from donorschoose.utils.common import read_csv\n",
    "from donorschoose.utils.common import read_stopwords_txt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_txt_file(path_to_txt: Path) -> list:\n",
    "    \"\"\"reads txt file and returns \n",
    "\n",
    "    Args:\n",
    "        path_to_txt (str): path like input\n",
    "\n",
    "    Raises:\n",
    "        e: empty file\n",
    "\n",
    "    Returns:\n",
    "       list\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(path_to_txt, \"r\") as f:\n",
    "            stopwords_string = f.read()\n",
    "\n",
    "        stopwords = [word.strip() for word in stopwords_string.split('\\n') if word.strip()]\n",
    "\n",
    "        logger.info(f\"stopword file: {path_to_txt}, Number of words :{len(stopwords)} loaded successfully\")\n",
    "        return  stopwords\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "def save_csv(data_frame,file_path ):\n",
    "    \"\"\"get size in KB\n",
    "\n",
    "    Args:\n",
    "        path (Path): path of the file\n",
    "\n",
    "    filtered_resource_data.to_csv('fi ltered_resource_data1000.csv', index=False) \n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    data_frame.to_csv(file_path, index=False) \n",
    "    logger.info(f\"save dataframe in {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaning:\n",
    "    def __init__(self, config: DataCleanConfig):\n",
    "        self.config = config\n",
    "        self.train_df = None\n",
    "        self.resource_df = None\n",
    "        self.stopwords= None\n",
    "\n",
    "    def read_files(self):\n",
    "        '''\n",
    "        Fetches data from the specified URLs and returns DataFrames.\n",
    "        '''\n",
    "        try: \n",
    "            root_dir = self.config.root_dir\n",
    "            os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "            train_path = Path(self.config.local_data_trainfile)\n",
    "            resource_path = Path(self.config.local_data_resourcefile)\n",
    "            stopword_txt_path = Path(self.config.local_data_stopwordsfile)\n",
    "            self.train_df = read_csv(train_path)\n",
    "            self.resource_df = read_csv(resource_path)\n",
    "            self.stopwords = read_txt_file(stopword_txt_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n",
    "    def clean_text_column(self, column:str ):\n",
    "        \"\"\"\n",
    "        this function will remove Spaces and special char from text column\n",
    "        \n",
    "        if we have any Nan value then it will fill nan value with \n",
    "        whatever have max count category\n",
    "        \"\"\"\n",
    "\n",
    "        self.train_df[column] =self.train_df[column].str.replace(' ','_')\n",
    "        self.train_df[column] =self.train_df[column].str.replace(',','_')\n",
    "        self.train_df[column] =self.train_df[column].str.replace('.','')\n",
    "        self.train_df[column] =self.train_df[column].str.replace('-','_')\n",
    "        self.train_df[column] =self.train_df[column].str.replace('&','_')\n",
    "        self.train_df[column] =self.train_df[column].str.replace(' The ','')\n",
    "        self.train_df[column] =self.train_df[column].str.lower()\n",
    "        self.train_df[column] =self.train_df[column].str.replace('___','_')\n",
    "        self.train_df[column] =self.train_df[column].str.replace('__','_')\n",
    "        \n",
    "        max_count_id = self.train_df[column].value_counts().idxmax()\n",
    "        self.train_df[column]=self.train_df[column].fillna(max_count_id)\n",
    "        \n",
    "        logger.info(f\"preprocessed column :{column} , unique categories Count : {len(self.train_df[column].value_counts())}\")\n",
    "\n",
    "\n",
    "    def add_two_column(self , column1: str , column2: str):\n",
    "        try:\n",
    "            if column1 not in self.train_df.columns:\n",
    "                self.train_df[column1] = self.train_df[column2].astype(str)\n",
    "            else:\n",
    "                self.train_df[column1] += self.train_df[column2].astype(str)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "    def decontracted(self ,phrase: str) -> str:\n",
    "        \n",
    "            # specific\n",
    "        phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "        # general\n",
    "        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "        \n",
    "        return phrase\n",
    "        \n",
    "\n",
    "    def preprocess_text(self ,column:str)-> list:\n",
    "        text_list = self.train_df[column].values\n",
    "        preprocessed_text = []\n",
    "        try:\n",
    "            for sentance in tqdm(text_list):\n",
    "                sent = self.decontracted(sentance)\n",
    "                sent = sent.replace('\\\\r', ' ')\n",
    "                sent = sent.replace('\\\\n', ' ')\n",
    "                sent = sent.replace('\\\\\"', ' ')\n",
    "                sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "                sent = ' '.join(e for e in sent.split() if e.lower() not in self.stopwords)\n",
    "                preprocessed_text.append(sent.lower().strip())\n",
    "            logger.info(f\"preprocessed column :{column}\")\n",
    "            self.train_df[column]=preprocessed_text\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def merge_csv(self):\n",
    "        \"\"\"\n",
    "        joining two dataframes in python\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.resource_df = self.resource_df.groupby('id').agg({'price':'sum', 'quantity':'sum'}).reset_index()\n",
    "            self.train_df =pd.merge(self.train_df, self.resource_df, on='id', how='left')\n",
    "            logger.info(f\"aggrigated resourde df and merged both df\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n",
    "    def normalize_column(self ,column: str):\n",
    "        try:\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(self.train_df[column].values.reshape(-1, 1))\n",
    "            self.train_df[column]=scaler.transform(self.train_df[column].values.reshape(-1, 1) )\n",
    "            logger.info(f\"normalized column : {column}\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "    def drop_colums(self , column_list : list):\n",
    "        self.train_df = self.train_df.drop(column_list, axis=1)\n",
    "        logger.info(f\"dropped columns:{column_list}\")\n",
    "        \n",
    "    def rename_column(self ,columnNanme:str , newName : str):\n",
    "        self.train_df = self.train_df.rename(columns={\n",
    "            columnNanme: newName,\n",
    "            })\n",
    "        logger.info(f\"renamed column:{columnNanme} to {newName}\")\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-09 14:53:24,429: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-02-09 14:53:24,433: INFO: common: created directory at: artifacts]\n",
      "{'root_dir': 'artifacts/data_cleaning', 'local_data_trainfile': 'artifacts\\\\data_ingestion\\\\dataset\\\\filtered_train_data1000.csv', 'local_data_resourcefile': 'artifacts\\\\data_ingestion\\\\dataset\\\\filtered_resource_data1000.csv', 'local_data_stopwordsfile': 'artifacts\\\\data_ingestion\\\\dataset\\\\stopwords-en.txt', 'save_clean_datafile': 'artifacts/data_cleaning/clean_data.csv'}\n",
      "[2024-02-09 14:53:24,437: INFO: common: created directory at: artifacts/data_cleaning]\n",
      "[2024-02-09 14:53:24,499: INFO: common: csv file: artifacts\\data_ingestion\\dataset\\filtered_train_data1000.csv, df Shape:(1000, 16) loaded successfully]\n",
      "[2024-02-09 14:53:24,516: INFO: common: csv file: artifacts\\data_ingestion\\dataset\\filtered_resource_data1000.csv, df Shape:(5650, 4) loaded successfully]\n",
      "[2024-02-09 14:53:24,531: INFO: 555177667: stopword file: artifacts\\data_ingestion\\dataset\\stopwords-en.txt, Number of words :1296 loaded successfully]\n",
      "[2024-02-09 14:53:24,555: INFO: 3771689221: aggrigated resourde df and merged both df]\n",
      "[2024-02-09 14:53:24,590: INFO: 3771689221: preprocessed column :project_subject_categories , unique categories Count : 38]\n",
      "[2024-02-09 14:53:24,650: INFO: 3771689221: preprocessed column :project_subject_subcategories , unique categories Count : 143]\n",
      "[2024-02-09 14:53:24,686: INFO: 3771689221: preprocessed column :project_grade_category , unique categories Count : 4]\n",
      "[2024-02-09 14:53:24,732: INFO: 3771689221: preprocessed column :teacher_prefix , unique categories Count : 4]\n",
      "[2024-02-09 14:53:24,800: INFO: 3771689221: preprocessed column :school_state , unique categories Count : 48]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 2006.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-09 14:53:25,310: INFO: 3771689221: preprocessed column :project_title]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 170.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-09 14:53:31,208: INFO: 3771689221: preprocessed column :essay]\n",
      "[2024-02-09 14:53:31,215: INFO: 3771689221: normalized column : price]\n",
      "[2024-02-09 14:53:31,219: INFO: 3771689221: normalized column : quantity]\n",
      "[2024-02-09 14:53:31,233: INFO: 3771689221: renamed column:project_subject_categories to clean_categories]\n",
      "[2024-02-09 14:53:31,234: INFO: 3771689221: renamed column:project_subject_subcategories to clean_subcategories]\n",
      "[2024-02-09 14:53:31,248: INFO: 3771689221: dropped columns:['project_essay_1', 'project_essay_2', 'project_essay_3', 'project_essay_4', 'teacher_id', 'project_submitted_datetime', 'id']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-09 14:53:31,534: INFO: 555177667: save dataframe in artifacts\\data_cleaning\\clean_data.csv]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "config = ConfigurationManager()\n",
    "data_clean_config = config.get_data_clean_config()\n",
    "data_clean = DataCleaning(config=data_clean_config)\n",
    "\n",
    "data_clean.read_files()\n",
    "data_clean.merge_csv()\n",
    "\n",
    "data_clean.clean_text_column(\"project_subject_categories\")\n",
    "data_clean.clean_text_column(\"project_subject_subcategories\")\n",
    "data_clean.clean_text_column(\"project_grade_category\")\n",
    "data_clean.clean_text_column(\"teacher_prefix\")\n",
    "data_clean.clean_text_column(\"school_state\")\n",
    "\n",
    "data_clean.preprocess_text(\"project_title\")\n",
    "\n",
    "data_clean.add_two_column(\"essay\" , \"project_essay_1\")\n",
    "data_clean.add_two_column(\"essay\" , \"project_essay_2\")\n",
    "data_clean.add_two_column(\"essay\" , \"project_essay_3\")\n",
    "data_clean.add_two_column(\"essay\" , \"project_essay_4\")\n",
    "data_clean.preprocess_text(\"essay\")\n",
    "data_clean.normalize_column(\"price\")\n",
    "data_clean.normalize_column(\"quantity\")\n",
    "data_clean.rename_column(\"project_subject_categories\" ,\"clean_categories\")\n",
    "data_clean.rename_column(\"project_subject_subcategories\" ,\"clean_subcategories\")\n",
    "drop_columns_list = [\"project_essay_1\",\n",
    "                \"project_essay_2\",\n",
    "                \"project_essay_3\",\n",
    "                \"project_essay_4\",\n",
    "                \"teacher_id\",\n",
    "                \"project_submitted_datetime\",\n",
    "                \"id\"]\n",
    "data_clean.drop_colums(drop_columns_list)\n",
    "save_csv(data_clean.train_df ,data_clean_config.save_clean_datafile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>school_state</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "      <th>essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mrs</td>\n",
       "      <td>in</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>esl_literacy</td>\n",
       "      <td>educational support for english learners at home</td>\n",
       "      <td>My students need opportunities to practice beg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.446746</td>\n",
       "      <td>0.236383</td>\n",
       "      <td>my students are english learners that are work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mr</td>\n",
       "      <td>fl</td>\n",
       "      <td>grades_6_8</td>\n",
       "      <td>history_civics_health_sports</td>\n",
       "      <td>civics_government_team_sports</td>\n",
       "      <td>wanted projector for hungry learners</td>\n",
       "      <td>My students need a projector to help with view...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.040664</td>\n",
       "      <td>-0.542591</td>\n",
       "      <td>our students arrive to our school eager to lea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  teacher_prefix school_state project_grade_category  \\\n",
       "0            mrs           in          grades_prek_2   \n",
       "1             mr           fl             grades_6_8   \n",
       "\n",
       "               clean_categories            clean_subcategories  \\\n",
       "0             literacy_language                   esl_literacy   \n",
       "1  history_civics_health_sports  civics_government_team_sports   \n",
       "\n",
       "                                      project_title  \\\n",
       "0  educational support for english learners at home   \n",
       "1              wanted projector for hungry learners   \n",
       "\n",
       "                            project_resource_summary  \\\n",
       "0  My students need opportunities to practice beg...   \n",
       "1  My students need a projector to help with view...   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "0                                             0                    0   \n",
       "1                                             7                    1   \n",
       "\n",
       "      price  quantity                                              essay  \n",
       "0 -0.446746  0.236383  my students are english learners that are work...  \n",
       "1 -0.040664 -0.542591  our students arrive to our school eager to lea...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'ve\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path_to_txt = Path(r\"E:\\parth\\DonorsChoose-Application-Screening-app\\artifacts\\data_ingestion\\dataset\\stopwords.txt\")\n",
    "# with open(path_to_txt, \"r\") as f:\n",
    "#     stopwords_string = f.read()\n",
    "\n",
    "# # Split the single string by newline characters and store the elements in a list\n",
    "# stopwords = [word.strip() for word in stopwords_string.split('\\n') if word.strip()]\n",
    "# stopwords[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_colums(self , column_list : list):\n",
    "    project_data = project_data.drop(column_list, axis=1)\n",
    "    \n",
    "def rename_column(columnNanme:str , newName : str):\n",
    "    project_data = project_data.rename(columns={\n",
    "        columnNanme: newName,\n",
    "        })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data = project_data.drop([\"project_essay_1\",\n",
    "                                  \"project_essay_2\",\n",
    "                                  \"project_essay_3\",\n",
    "                                  \"project_essay_4\",\n",
    "                                  \"teacher_id\",\n",
    "                                  \"Unnamed: 0\",\n",
    "                                  \"project_submitted_datetime\",\n",
    "                                  \"id\"], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "project_data = project_data.rename(columns={'project_subject_categories': 'clean_categories',\n",
    "                        'project_subject_subcategories': 'clean_subcategories'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcenv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
