{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\parth\\\\DonorsChoose-Application-Screening-app'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "Project_path = \"E:\\parth\\DonorsChoose-Application-Screening-app\"\n",
    "os.chdir(Project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\parth\\\\DonorsChoose-Application-Screening-app'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'config/config.yaml'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"config/config.yaml\"\"\"\n",
    "\n",
    "# artifacts_root: artifacts\n",
    "\n",
    "# data_ingestion:\n",
    "#   root_dir: artifacts/data_ingestion\n",
    "#   source_URL: https://drive.google.com/file/d/1-4mgFvbEEvCuIar7eas29F5k8hfbRFn3/view?usp=sharing\n",
    "#   local_data_file: artifacts/data_ingestion/dataset.zip\n",
    "#   unzip_dir: artifacts/data_ingestion\n",
    "\n",
    "# data_cleaning:\n",
    "#   root_dir: artifacts/data_cleaning\n",
    "#   local_data_trainfile: artifacts/data_ingestion/dataset/train1000.csv\n",
    "#   local_data_resourcefile: artifacts/data_ingestion/dataset/resource1000.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"src/entity/config_entity -  added entity for data clean \"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataCleanConfig:\n",
    "    root_dir: Path\n",
    "    local_data_trainfile: Path\n",
    "    local_data_resourcefile: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"data config manager it will create config object for data clean \"\"\"\n",
    "\n",
    "from donorschoose.constants import *\n",
    "from donorschoose.utils.common import read_yaml, create_directories\n",
    "# from donorschoose.entity.config_entity import DataCleanConfig\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    \n",
    "    def get_data_clean_config(self) -> DataCleanConfig:\n",
    "        config = self.config.data_cleaning\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_clean_config = DataCleanConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            local_data_trainfile=config.local_data_trainfile,\n",
    "            local_data_resourcefile=config.local_data_resourcefile\n",
    "        )\n",
    "        return data_clean_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"common function for data rread\"\"\"\n",
    "import os\n",
    "from box.exceptions import BoxValueError\n",
    "import yaml\n",
    "from donorschoose import logger\n",
    "import json\n",
    "import joblib\n",
    "from ensure import ensure_annotations\n",
    "from box import ConfigBox\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "import pandas as pd\n",
    "\n",
    "@ensure_annotations\n",
    "def read_csv(path_to_csv: Path) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"reads csv file and returns \n",
    "\n",
    "    Args:\n",
    "        path_to_csv (str): path like input\n",
    "\n",
    "    Raises:\n",
    "        e: empty file\n",
    "\n",
    "    Returns:\n",
    "        pd.core.frame.DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(path_to_csv) as csv_file:\n",
    "            dataframe = pd.read_csv(csv_file)\n",
    "            logger.info(f\"csv file: {path_to_csv}, df Shape:{dataframe.shape} loaded successfully\")\n",
    "            return dataframe\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-09 00:24:48,890: INFO: 2445554870: csv file: E:\\parth\\DonorsChoose-Application-Screening-app\\artifacts\\data_ingestion\\dataset\\train1000.csv, df Shape:(1000, 17) loaded successfully]\n",
      "(1000, 17)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "csv_path = Path(Project_path + r\"\\artifacts\\data_ingestion\\dataset\\train1000.csv\")\n",
    "df = read_csv(csv_path)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[159], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"component\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mTuple\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdonorschoose\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Tuple'"
     ]
    }
   ],
   "source": [
    "\"\"\"component\"\"\"\n",
    "\n",
    "import os\n",
    "from donorschoose import logger\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DataCleaning:\n",
    "    def __init__(self, config: DataCleanConfig):\n",
    "        self.config = config\n",
    "        self.stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "    def read_files(self) -> pd.core.frame.DataFrame:\n",
    "        '''\n",
    "        Fetches data from the specified URLs and returns DataFrames.\n",
    "        '''\n",
    "        try: \n",
    "            root_dir = self.config.root_dir\n",
    "            os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "            train_path = self.config.local_data_trainfile\n",
    "            resource_path = self.config.local_data_resourcefile\n",
    "            train_df = read_csv(train_path)\n",
    "            resource_df = read_csv(resource_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "        return train_df, resource_df\n",
    "    \n",
    "    def clean_text_column(df_column :pd.core.frame.DataFrame , columnName : str  )-> pd.core.frame.DataFrame :\n",
    "        \"\"\"\n",
    "        this function will remove Spaces and special char from text column\n",
    "        \n",
    "        if we have any Nan value then it will fill nan value with \n",
    "        whatever have max count category\n",
    "        \"\"\"\n",
    "        df_column =df_column.str.replace(' ','_')\n",
    "        df_column =df_column.str.replace(',','_')\n",
    "        df_column =df_column.str.replace('.','')\n",
    "        df_column =df_column.str.replace('-','_')\n",
    "        df_column =df_column.str.replace('&','_')\n",
    "        df_column =df_column.str.replace(' The ','')\n",
    "        df_column =df_column.str.lower()\n",
    "        df_column =df_column.str.replace('___','_')\n",
    "        df_column =df_column.str.replace('__','_')\n",
    "        \n",
    "        max_count_id = df_column.value_counts().idxmax()\n",
    "        df_column=df_column.fillna(max_count_id)\n",
    "        \n",
    "        logger.info(f\"preprocessed column :{columnName} , unique categories Count : {len(df_column.value_counts())}\")\n",
    "\n",
    "        return df_column\n",
    "\n",
    "    def decontracted(phrase: str) -> str:\n",
    "        # specific\n",
    "        phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "        # general\n",
    "        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "        \n",
    "        return phrase\n",
    "\n",
    "    def preprocess_text(text_data: list)-> list:\n",
    "        preprocessed_text = []\n",
    "        for sentance in tqdm(text_data):\n",
    "            sent = decontracted(sentance)\n",
    "            sent = sent.replace('\\\\r', ' ')\n",
    "            sent = sent.replace('\\\\n', ' ')\n",
    "            sent = sent.replace('\\\\\"', ' ')\n",
    "            sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "            sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n",
    "            preprocessed_text.append(sent.lower().strip())\n",
    "        return preprocessed_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-09 00:35:42,529: INFO: 2445554870: csv file: E:\\parth\\DonorsChoose-Application-Screening-app\\artifacts\\data_ingestion\\dataset\\train1000.csv, df Shape:(1000, 17) loaded successfully]\n",
      "[2024-02-09 00:35:42,617: INFO: 2445554870: csv file: E:\\parth\\DonorsChoose-Application-Screening-app\\artifacts\\data_ingestion\\dataset\\resource1000.csv, df Shape:(1000, 17) loaded successfully]\n"
     ]
    }
   ],
   "source": [
    "project_csv_path = Path(Project_path + r\"\\artifacts\\data_ingestion\\dataset\\train1000.csv\")\n",
    "resource_csv_path = Path(Project_path + r\"\\artifacts\\data_ingestion\\dataset\\resource1000.csv\")\n",
    "project_data = read_csv(csv_path)\n",
    "resource_data =read_csv(resource_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-09 00:35:45,015: INFO: 3332820451: preprocessed column :project_subject_categories , unique category count: 38]\n",
      "[2024-02-09 00:35:45,097: INFO: 3332820451: preprocessed column :project_subject_subcategories , unique category count: 143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-09 00:35:45,145: INFO: 3332820451: preprocessed column :teacher_prefix , unique category count: 4]\n",
      "[2024-02-09 00:35:45,189: INFO: 3332820451: preprocessed column :school_state , unique category count: 48]\n"
     ]
    }
   ],
   "source": [
    "project_data['project_grade_category'] = clean_text_column(project_data['project_subject_categories'] ,'project_subject_categories')\n",
    "project_data['project_subject_categories'] = clean_text_column(project_data['project_subject_subcategories'] ,'project_subject_subcategories')\n",
    "project_data['teacher_prefix'] = clean_text_column(project_data['teacher_prefix'] ,'teacher_prefix')\n",
    "project_data['school_state'] = clean_text_column(project_data['school_state'] ,'school_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_subject_categories\n",
    "#project_subject_subcategories\n",
    "#teacher_prefix\n",
    "#school_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1403.68it/s]\n"
     ]
    }
   ],
   "source": [
    "project_data['project_title'] = preprocess_text(project_data['project_title'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data[\"essay\"] = project_data[\"project_essay_1\"].map(str) +\\\n",
    "                        project_data[\"project_essay_2\"].map(str) + \\\n",
    "                        project_data[\"project_essay_3\"].map(str) + \\\n",
    "                        project_data[\"project_essay_4\"].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:05<00:00, 185.53it/s]\n"
     ]
    }
   ],
   "source": [
    "project_data['essay']= preprocess_text(project_data['essay'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data = resource_data.groupby('id').agg({'price':'sum', 'quantity':'sum'}).reset_index()\n",
    "project_data = pd.merge(project_data, price_data, on='id', how='left')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(project_data['price'].values.reshape(-1, 1))\n",
    "project_data['price']=scaler.transform(project_data['price'].values.reshape(-1, 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data = project_data.drop([\"project_essay_1\",\n",
    "                                  \"project_essay_2\",\n",
    "                                  \"project_essay_3\",\n",
    "                                  \"project_essay_4\",\n",
    "                                  \"teacher_id\",\n",
    "                                  \"Unnamed: 0\",\n",
    "                                  \"project_submitted_datetime\",\n",
    "                                  \"id\"], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "project_data = project_data.rename(columns={'project_subject_categories': 'clean_categories',\n",
    "                        'project_subject_subcategories': 'clean_subcategories'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcenv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
